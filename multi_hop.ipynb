{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import cast\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import Field, conlist, create_model\n",
    "import torch\n",
    "import string\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbaa65-8b00-402b-b599-270b2738c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or set environment variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21278a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "K_RETRIEVE = 5\n",
    "MAX_HOPS = 3 # realistcally only 2 hops needed for these datasets\n",
    "MAX_DOCS_PER_HOP = 3\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HotpotQA: use TRAIN for corpus, VALIDATION for evaluation\n",
    "print(\"Loading HotpotQA...\")\n",
    "ds_train = cast(Dataset, load_dataset(\"hotpot_qa\", \"distractor\", split=\"train\", streaming=False)) # cast to Dataset to avoid pylance error\n",
    "ds_val = cast(Dataset, load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\", streaming=False))\n",
    "\n",
    "print(\"Train size:\", len(ds_train))\n",
    "print(\"Validation size:\", len(ds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba631161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus from training and validation sets\n",
    "corpus_rows = []\n",
    "for example in ds_train:\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sentences_lists = example[\"context\"][\"sentences\"]\n",
    "    for title, sents in zip(titles, sentences_lists):\n",
    "        paragraph_text = \" \".join(sents)\n",
    "        corpus_rows.append({\"title\": title, \"text\": paragraph_text})\n",
    "\n",
    "for example in ds_val:\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sentences_lists = example[\"context\"][\"sentences\"]\n",
    "    for title, sents in zip(titles, sentences_lists):\n",
    "        paragraph_text = \" \".join(sents)\n",
    "        corpus_rows.append({\"title\": title, \"text\": paragraph_text})\n",
    "\n",
    "# Remove duplicates\n",
    "unique_seen = set()\n",
    "unique_rows = []\n",
    "for row in corpus_rows:\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", row[\"text\"]).strip().lower()\n",
    "    key = (row[\"title\"], clean_text)\n",
    "    if key not in unique_seen:\n",
    "        unique_seen.add(key)\n",
    "        unique_rows.append({\"title\": row[\"title\"], \"text\": row[\"text\"]})\n",
    "\n",
    "corpus_rows = unique_rows\n",
    "print(\"Paragraphs:\", len(corpus_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk with RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "texts, metas = [], []\n",
    "for r in corpus_rows:\n",
    "    chunks = text_splitter.split_text(r['text'])\n",
    "    texts.extend(chunks)\n",
    "    metas.extend([{\"title\": r['title']} for _ in chunks])\n",
    "\n",
    "print(\"Chunks indexed:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build or load FAISS vector store (TODO: move this and the code b4 to a separate script to reuse later)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": device}, # Use GPU if available\n",
    "    encode_kwargs={\"normalize_embeddings\": True}, \n",
    "    # show_progress=True\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_hotpotqa\"):\n",
    "    print(\"Loading existing FAISS vector store from faiss_hotpotqa...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_hotpotqa\", embedding_model, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Creating new FAISS vector store...\")\n",
    "    vector_store = FAISS.from_texts(\n",
    "        texts,\n",
    "        embedding_model, \n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    # Save vector store to disk for future use\n",
    "    vector_store.save_local(\"faiss_hotpotqa\")\n",
    "    print(\"FAISS vector store saved to faiss_hotpotqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM question decomposition\n",
    "# Build a structured LLM that enforces: 2..max_subqs subquestions\n",
    "def make_decomposer(llm, max_subqs=MAX_HOPS):\n",
    "    # Dynamic schema so you can set max_subqs at runtime\n",
    "    DecompSchema = create_model(\n",
    "        \"DecompSchema\",\n",
    "        subquestions=(conlist(str, min_length=2, max_length=max_subqs),\n",
    "                      Field(description=\"Ordered sub-questions to solve the original in sequence.\"))\n",
    "    )\n",
    "\n",
    "    # Use strict structured output (no heuristics, no fallback)\n",
    "    structured_llm = llm.bind_tools(\n",
    "        tools=[],                     # no tools needed; we just want the schema\n",
    "        response_format=DecompSchema, # pydantic schema\n",
    "        strict=True\n",
    "    )\n",
    "\n",
    "    SYSTEM = \"You have to break complex questions into concise, sequential sub-questions.\"\n",
    "    USER_TMPL = (\n",
    "        f\"- Produce BETWEEN 2 and {max_subqs} sub-questions that will help answer the main question.\\n\"\n",
    "        \"- Each sub-question MUST be under 18 words.\\n\"\n",
    "        \"- Each sub-question must be specific and answerable using a retrieval system.\\n\"\n",
    "        \"- Each sub-question must contribute useful information towards answering the main question.\\n\"\n",
    "        \"- Answers to sub-questions must solve the main question when combined.\\n\"\n",
    "        \"- Order the list so answering in order solves the original question.\\n\"\n",
    "        \"- No extra keys. No commentary. No markdown.\\n\\n\"\n",
    "        \"QUESTION: {q}\"\n",
    "    )\n",
    "\n",
    "    def decompose(question):\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": USER_TMPL.format(q=question)},\n",
    "        ]\n",
    "        resp = structured_llm.invoke(msgs)\n",
    "        # LangChain stores the parsed Pydantic object here:\n",
    "        parsed = resp.additional_kwargs[\"parsed\"]\n",
    "        # -> parsed is a dict with key \"subquestions\"\n",
    "        return parsed.subquestions\n",
    "\n",
    "    return decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for multi-hop QA\n",
    "def compose_query(llm, question, subq, hops):\n",
    "    SYSTEM_PROMPT = (\"You are to rewrite questions into focused search queries for retrieval.\")\n",
    "    mem_lines = \"\\n\".join([f\"{i+1}. {h['subq']} -> {h['answer']}\" for i, h in enumerate(hops)]) or \"None yet.\"\n",
    "    user_prompt = (\n",
    "        \"Rewrite the following sub-question into a concise search query for document retrieval.\\n\"\n",
    "        \"- use entities/names filled by PRIOR ANSWERS when relevant.\\n\"\n",
    "        \"- if prior answers don't help, keep the original sub-question details.\\n\"\n",
    "        \"- Keep it under 18 words. No pronuns like this/that/it.\\n\"\n",
    "        \"- The query must remain as a question.\\n\"\n",
    "        \"- Be specific and retrieval-friendly (names, years, titles).\\n\\n\"\n",
    "        f\"ORIGINAL QUESTION: {question}\\n\\n\"\n",
    "        f\"PRIOR ANSWERS:\\n{mem_lines}\\n\\n\"\n",
    "        f\"SUB-QUESTION: {subq}\\n\\n\"\n",
    "        \"SEARCH QUERY:\"\n",
    "    )\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    return resp.content.strip()\n",
    "\n",
    "def answer_subq(llm, question, subq, passages, hops):\n",
    "    SYSTEM_PROMPT = (\"You are a precise QA assistant. Return only the short answer phrase. No explanation, no full sentences.\")\n",
    "    mem_lines = \"\\n\".join([f\"{i+1}. {h['subq']} -> {h['answer']}\" for i, h in enumerate(hops)]) or \"None.\"\n",
    "    ctx = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{p}\" for i, p in enumerate(passages)])\n",
    "    user_prompt = (\n",
    "        \"Answer the CURRENT SUB-QUESTION using the CONTEXT passages provided.\\n\"\n",
    "        f\"ORIGINAL QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"PRIOR ANSWERS:\\n{mem_lines}\\n\\n\"\n",
    "        f\"CURRENT SUB-QUESTION:\\n{subq}\\n\\n\"\n",
    "        f\"CONTEXT:\\n{ctx}\\n\\n\"\n",
    "        \"Answer (short phrase only):\"\n",
    "    )\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    return resp.content.strip()\n",
    "\n",
    "def get_final_answer(llm, question, hops):\n",
    "    SYSTEM_PROMPT = (\"You are a precise QA assistant. Return only the short answer phrase (in some cases 1-2 words will suffice). No explanation, no full sentences.\")\n",
    "    mem_lines = \"\\n\".join([f\"{i+1}. {h['subq']} -> {h['answer']}\" for i, h in enumerate(hops)]) or \"None.\"\n",
    "    support = []\n",
    "    for i, h in enumerate(hops):\n",
    "        for j, p in enumerate(h['passages']):\n",
    "            support.append(f\"HOP {i+1} PASSAGE {j+1}:\\n{p}\")\n",
    "    ctx = \"\\n\\n\".join(support)\n",
    "    user_prompt = (\n",
    "        \"Using the prior sub-questions and their answers along with the supporting context, answer the ORIGINAL QUESTION.\\n\"\n",
    "        f\"ORIGINAL QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"SUB-QUESTION ANSWERS:\\n{mem_lines}\\n\\n\"\n",
    "        f\"SUPPORTING CONTEXT:\\n{ctx}\\n\\n\"\n",
    "        \"Final answer (short phrase only):\"\n",
    "    )\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    return resp.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hop QA Pipeline\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
    "decomposer = make_decomposer(llm, MAX_HOPS)\n",
    "\n",
    "def multi_hop_qa(question, max_docs_per_hop=MAX_DOCS_PER_HOP, k=K_RETRIEVE):\n",
    "    subquestions = decomposer(question)\n",
    "    hops = [] # each hop will be {\"subq\":..., \"composed\":..., \"passages\":..., \"answer\":...}\n",
    "    \n",
    "    for subq in subquestions:\n",
    "        # Recompose query using previous answers if any\n",
    "        composed_q = \"\"\n",
    "        if hops:\n",
    "            composed_q = compose_query(llm, question, subq, hops)\n",
    "        else:\n",
    "            composed_q = subq\n",
    "\n",
    "        # Retrieve documents for this sub-question\n",
    "        retrieved = vector_store.similarity_search(composed_q, k=k)\n",
    "        passages = [doc.page_content for doc in retrieved][:max_docs_per_hop]\n",
    "\n",
    "        # Answer current hop\n",
    "        ans = answer_subq(llm, question, subq, passages, hops)\n",
    "        hops.append({\n",
    "            \"subq\": subq,\n",
    "            \"composed\": composed_q,\n",
    "            \"passages\": passages,\n",
    "            \"answer\": ans\n",
    "        })\n",
    "\n",
    "    # get the final answer using passages and answers from all hops\n",
    "    final_answer = get_final_answer(llm, question, hops)\n",
    "    return final_answer, hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test on one example\n",
    "query = \"What is the nickname of the city where Darling's Waterfront Pavilion is located?\"\n",
    "pred, hops = multi_hop_qa(query, MAX_DOCS_PER_HOP, K_RETRIEVE)\n",
    "print(\"Question:\", query)\n",
    "for i, h in enumerate(hops):\n",
    "    print(f\"Sub-question {i + 1}: {h['subq']}\")\n",
    "    print(f\"Composed Query: {h['composed']}\")\n",
    "    for j, p in enumerate(h['passages']):\n",
    "        print(f\"Passage {j + 1}:\\n{p}\\n\")\n",
    "    print(f\"Answer: {h['answer']}\\n\")\n",
    "\n",
    "print(\"Original Question:\", query)\n",
    "print(\"Predicted Answer:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM/F1 evaluation\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "    if normalized_ground_truth in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return 1.0 if (normalize_answer(prediction) == normalize_answer(ground_truth)) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde64dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(ds, n, k=K_RETRIEVE):\n",
    "    idxs = list(range(min(n, len(ds)))) # first n examples\n",
    "\n",
    "    ems, f1s = [], []\n",
    "\n",
    "    for i in idxs:\n",
    "        ex = ds[i]\n",
    "        q = ex[\"question\"]\n",
    "        ground_truth = ex[\"answer\"]\n",
    "\n",
    "        # Predictions from your singlehop system\n",
    "        pred, _ = multi_hop_qa(q, MAX_DOCS_PER_HOP, k=k)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"Pred: {pred}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "\n",
    "        ems.append(exact_match_score(pred, ground_truth))\n",
    "        f1s.append(f1_score(pred, ground_truth))\n",
    "\n",
    "    m = len(idxs) if idxs else 1\n",
    "    return {\n",
    "        \"n\": len(idxs),\n",
    "        \"k\": k,\n",
    "        \"EM\": sum(ems)/m,\n",
    "        \"F1\": sum(f1s)/m,\n",
    "    }\n",
    "\n",
    "# Run eval\n",
    "metrics = eval(ds_val, 100, k=K_RETRIEVE) # TODO: change N to ds_val size for full eval later\n",
    "print(\"Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
