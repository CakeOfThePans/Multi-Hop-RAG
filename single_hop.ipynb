{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1a13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import cast\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import string\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bbaa65-8b00-402b-b599-270b2738c779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load or set environment variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21278a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Config (tweak these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "K_RETRIEVE = 5\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f36bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HotpotQA...\n",
      "Train size: 90447\n",
      "Validation size: 7405\n"
     ]
    }
   ],
   "source": [
    "# Load HotpotQA: use TRAIN for corpus, VALIDATION for evaluation\n",
    "print(\"Loading HotpotQA...\")\n",
    "ds_train = cast(Dataset, load_dataset(\"hotpot_qa\", \"distractor\", split=\"train\", streaming=False)) # cast to Dataset to avoid pylance error\n",
    "ds_val = cast(Dataset, load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\", streaming=False))\n",
    "\n",
    "print(\"Train size:\", len(ds_train))\n",
    "print(\"Validation size:\", len(ds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba631161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 482021\n"
     ]
    }
   ],
   "source": [
    "# Build a corpus from TRAIN context only\n",
    "corpus_rows = []\n",
    "for example in ds_train:\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sentences_lists = example[\"context\"][\"sentences\"]\n",
    "    for title, sents in zip(titles, sentences_lists):\n",
    "        paragraph_text = \" \".join(sents)\n",
    "        corpus_rows.append({\"title\": title, \"text\": paragraph_text})\n",
    "\n",
    "# Remove duplicates\n",
    "unique_seen = set()\n",
    "unique_rows = []\n",
    "for row in corpus_rows:\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", row[\"text\"]).strip().lower()\n",
    "    key = (row[\"title\"], clean_text)\n",
    "    if key not in unique_seen:\n",
    "        unique_seen.add(key)\n",
    "        unique_rows.append({\"title\": row[\"title\"], \"text\": row[\"text\"]})\n",
    "\n",
    "corpus_rows = unique_rows\n",
    "print(\"Paragraphs:\", len(corpus_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cf66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks indexed: 505435\n"
     ]
    }
   ],
   "source": [
    "# Chunk with RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "texts, metas = [], []\n",
    "for r in corpus_rows:\n",
    "    chunks = text_splitter.split_text(r['text'])\n",
    "    texts.extend(chunks)\n",
    "    metas.extend([{\"title\": r['title']} for _ in chunks])\n",
    "\n",
    "print(\"Chunks indexed:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2740b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS vector store from faiss_hotpotqa...\n"
     ]
    }
   ],
   "source": [
    "# Build or load FAISS vector store (TODO: move this and the code b4 to a separate script to reuse later)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": device}, # Use GPU if available\n",
    "    encode_kwargs={\"normalize_embeddings\": True}, \n",
    "    # show_progress=True\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_hotpotqa\"):\n",
    "    print(\"Loading existing FAISS vector store from faiss_hotpotqa...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_hotpotqa\", embedding_model, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Creating new FAISS vector store...\")\n",
    "    vector_store = FAISS.from_texts(\n",
    "        texts,\n",
    "        embedding_model, \n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    # Save vector store to disk for future use\n",
    "    vector_store.save_local(\"faiss_hotpotqa\")\n",
    "    print(\"FAISS vector store saved to faiss_hotpotqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75c5713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "First for Women is a woman's magazine published by Bauer Media Group in the USA.  The magazine was started in 1989.  It is based in Englewood Cliffs, New Jersey.  In 2011 the circulation of the magazine was 1,310,696 copies.\n",
      "Metadata: {'title': 'First for Women'}\n",
      "\n",
      "Document 2:\n",
      "Arthur's Home Magazine (1852-ca.1898) or Ladies' Home Magazine was an American periodical published in Philadelphia by Timothy Shay Arthur.  Editors Arthur and Virginia Francis Townsend selected writing and illustrations intended to appeal to female readers.  Among the contributors: Mary Tyler Peabody Mann and Kate Sutherland.  In its early years the monthly comprised a selection of articles originally published in Arthur's weekly \"Home Gazette.\"  Its nonfiction stories contained occasional factual inaccuracies for the sake of a good read.  A contemporary review judged it \"gotten up in good taste and well; and is in nothing overdone.  Even its fashion plates are not quite such extravagant caricatures of rag-baby work as are usually met with in some of the more fancy magazines.\"  Readers included patrons of the Mercantile Library Association of San Francisco.\n",
      "Metadata: {'title': \"Arthur's Lady's Home Magazine\"}\n",
      "\n",
      "Document 3:\n",
      "Arthur's Magazine (1844â€“1846) was an American literary periodical published in Philadelphia in the 19th century.  Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.  In May 1846 it was merged into \"Godey's Lady's Book\".\n",
      "Metadata: {'title': \"Arthur's Magazine\"}\n",
      "\n",
      "Document 4:\n",
      "Arthur magazine was a bi-monthly periodical that was founded in October 2002, by publisher Laris Kreslins and editor Jay Babcock.  It received favorable attention from other periodicals such as \"L.A. Weekly\", \"Print\", \"Punk Planet\" and \"Rolling Stone\".  \"Arthur\" featured photography and artwork from Spike Jonze, Art Spiegelman, Susannah Breslin, Gary Panter and Godspeed You!  Black Emperor.  Arthur's regular columnists included Byron Coley, Thurston Moore, Daniel Pinchbeck, Paul Cullum, Douglas Rushkoff, and T-Model Ford.\n",
      "Metadata: {'title': 'Arthur (magazine)'}\n",
      "\n",
      "Document 5:\n",
      "womenSports magazine was the first magazine dedicated to women in sports.  It was launched in close conjunction with Billie Jean King's Women's Sports Foundation and each issue of the magazine contained a two-page article written by the executive director of the Foundation.\n",
      "Metadata: {'title': 'WomenSports'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test retrieval on 1 example\n",
    "query = \"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
    "docs = vector_store.similarity_search(query, k=5)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3e933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer with LLM + Retrieved Docs\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
    "\n",
    "SYSTEM_PROMPT = (\"You are a precise QA assistant. Return just the short answer phrase (no explanation, no full sentence).\")\n",
    "\n",
    "def build_user_prompt(question, passages):\n",
    "    bundle = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{p}\" for i, p in enumerate(passages)])\n",
    "    return f\"{bundle}\\n\\nQUESTION: {question}\\nANSWER:\"\n",
    "\n",
    "def singlehop_answer(question, k = K_RETRIEVE):\n",
    "    docs = vector_store.similarity_search(question, k=k)\n",
    "    # Keep only the page content to reduce tokens\n",
    "    passages = [d.page_content for d in docs]\n",
    "    user_prompt = build_user_prompt(question, passages)\n",
    "    resp = llm.invoke([{\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "                       {\"role\":\"user\",\"content\": user_prompt}])\n",
    "    pred = resp.content\n",
    "    return pred, passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46af9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM/F1 evaluation\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "    if normalized_ground_truth in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return 1.0 if (normalize_answer(prediction) == normalize_answer(ground_truth)) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde64dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'n': 100, 'k': 5, 'EM': 0.39, 'F1': 0.46769191919191916}\n"
     ]
    }
   ],
   "source": [
    "def eval(ds, n, k=K_RETRIEVE):\n",
    "    # idxs = list(range(min(n, len(ds)))) # first n examples\n",
    "    idxs = random.sample(range(len(ds)), min(n, len(ds)))  # random n examples\n",
    "\n",
    "    ems, f1s = [], []\n",
    "\n",
    "    for i in idxs:\n",
    "        ex = ds[i]\n",
    "        q = ex[\"question\"]\n",
    "        ground_truth = ex[\"answer\"]\n",
    "\n",
    "        # Predictions from your singlehop system\n",
    "        pred, _ = singlehop_answer(q, k=k)\n",
    "        # print(f\"Q: {q}\")\n",
    "        # print(f\"Pred: {pred}\")\n",
    "        # print(f\"Ground Truth: {ground_truth}\")\n",
    "\n",
    "        ems.append(exact_match_score(pred, ground_truth))\n",
    "        f1s.append(f1_score(pred, ground_truth))\n",
    "\n",
    "    m = len(idxs) if idxs else 1\n",
    "    return {\n",
    "        \"n\": len(idxs),\n",
    "        \"k\": k,\n",
    "        \"EM\": sum(ems)/m,\n",
    "        \"F1\": sum(f1s)/m,\n",
    "    }\n",
    "\n",
    "# Run eval\n",
    "metrics = eval(ds_val, 100, k=K_RETRIEVE) # TODO: change N to ds_val size for full eval later\n",
    "print(\"Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
