{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import cast\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bbaa65-8b00-402b-b599-270b2738c779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load or set environment variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21278a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Config (tweak these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "K_RETRIEVE = 5\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f36bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HotpotQA...\n",
      "Train size: 90447\n",
      "Validation size: 7405\n"
     ]
    }
   ],
   "source": [
    "# Load HotpotQA: use TRAIN for corpus, VALIDATION for evaluation\n",
    "print(\"Loading HotpotQA...\")\n",
    "ds_train = cast(Dataset, load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train\", streaming=False)) # cast to Dataset to avoid pylance error\n",
    "ds_val = cast(Dataset, load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\", streaming=False))\n",
    "\n",
    "print(\"Train size:\", len(ds_train))\n",
    "print(\"Validation size:\", len(ds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba631161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 508826\n"
     ]
    }
   ],
   "source": [
    "# Build a corpus from training and validation sets\n",
    "corpus_rows = []\n",
    "for example in ds_train:\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sentences_lists = example[\"context\"][\"sentences\"]\n",
    "    for title, sents in zip(titles, sentences_lists):\n",
    "        paragraph_text = \" \".join(sents)\n",
    "        corpus_rows.append({\"title\": title, \"text\": paragraph_text})\n",
    "\n",
    "for example in ds_val:\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sentences_lists = example[\"context\"][\"sentences\"]\n",
    "    for title, sents in zip(titles, sentences_lists):\n",
    "        paragraph_text = \" \".join(sents)\n",
    "        corpus_rows.append({\"title\": title, \"text\": paragraph_text})\n",
    "\n",
    "# Remove duplicates\n",
    "unique_seen = set()\n",
    "unique_rows = []\n",
    "for row in corpus_rows:\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", row[\"text\"]).strip().lower()\n",
    "    key = (row[\"title\"], clean_text)\n",
    "    if key not in unique_seen:\n",
    "        unique_seen.add(key)\n",
    "        unique_rows.append({\"title\": row[\"title\"], \"text\": row[\"text\"]})\n",
    "\n",
    "corpus_rows = unique_rows\n",
    "print(\"Paragraphs:\", len(corpus_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cf66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks indexed: 533179\n"
     ]
    }
   ],
   "source": [
    "# Chunk with RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "texts, metas = [], []\n",
    "for r in corpus_rows:\n",
    "    chunks = text_splitter.split_text(r['text'])\n",
    "    texts.extend(chunks)\n",
    "    metas.extend([{\"title\": r['title']} for _ in chunks])\n",
    "\n",
    "print(\"Chunks indexed:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2740b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS vector store from faiss_hotpotqa...\n"
     ]
    }
   ],
   "source": [
    "# Build or load FAISS vector store (TODO: move this and the code b4 to a separate script to reuse later)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": device}, # Use GPU if available\n",
    "    encode_kwargs={\"normalize_embeddings\": True}, \n",
    "    # show_progress=True\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_hotpotqa\"):\n",
    "    print(\"Loading existing FAISS vector store from faiss_hotpotqa...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_hotpotqa\", embedding_model, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Creating new FAISS vector store...\")\n",
    "    vector_store = FAISS.from_texts(\n",
    "        texts,\n",
    "        embedding_model, \n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    # Save vector store to disk for future use\n",
    "    vector_store.save_local(\"faiss_hotpotqa\")\n",
    "    print(\"FAISS vector store saved to faiss_hotpotqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3e933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer with LLM + Retrieved Docs\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise QA assistant. Return just the short answer phrase with no explanation, and no full sentences.\"\n",
    "    \"If you are COMPLETELY UNSURE of the answer based on the provided passages, respond with 'Unknown'.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(question, passages):\n",
    "    bundle = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{p}\" for i, p in enumerate(passages)])\n",
    "    return f\"{bundle}\\n\\nQUESTION: {question}\\nANSWER:\"\n",
    "\n",
    "def singlehop_answer(question, k = K_RETRIEVE):\n",
    "    docs = vector_store.similarity_search(question, k=k)\n",
    "    # Keep only the page content to reduce tokens\n",
    "    passages = [d.page_content for d in docs]\n",
    "    user_prompt = build_user_prompt(question, passages)\n",
    "    resp = llm.invoke([{\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "                       {\"role\":\"user\",\"content\": user_prompt}])\n",
    "    pred = resp.content\n",
    "    return pred, passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f560bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Predicted Answer: Yes\n",
      "Retrieved Passages:\n",
      "Passage 1:\n",
      "Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.  He lives in Los Angeles, California.  He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange.\"\n",
      "\n",
      "Passage 2:\n",
      "Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
      "\n",
      "Passage 3:\n",
      "Donald G. Jackson (April 24, 1943 – October 20, 2003) was an American filmmaker who is often referred to in the media as the Ed Wood of the video age.  This delination was given due to the bizarre nature, content, and lack of defined storyline prevalent in his film and because virtually all of his films were harshly criticized by film critics.\n",
      "\n",
      "Passage 4:\n",
      "Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\n",
      "\n",
      "Passage 5:\n",
      "Adrian Charles \"Ade\" Edmondson (born 24 January 1957) is an English comedian, actor, writer, musician, television presenter and director.  He came to prominence in the early 1980s and was part of the alternative comedy boom.  He is probably best known for his comedic roles in the television series \"The Young Ones\" (1982–84) and \"Bottom\" (1991–95), which he wrote together with his long-time collaborative partner Rik Mayall.  Edmondson also appeared in \"The Comic Strip Presents...\" series of films throughout the 1980s and 1990s.  For one episode of this he created the spoof heavy metal band Bad News, and for another he played his nihilistic alter-ego Eddie Monsoon, an offensive South African television star.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test on one example\n",
    "query = \"Were Scott Derrickson and Ed Wood of the same nationality?\"\n",
    "pred, passages = singlehop_answer(query, k=K_RETRIEVE)\n",
    "print(\"Question:\", query)\n",
    "print(\"Predicted Answer:\", pred)\n",
    "print(\"Retrieved Passages:\")\n",
    "for i, p in enumerate(passages):\n",
    "    print(f\"Passage {i+1}:\\n{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46af9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM/F1 evaluation\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "    if normalized_ground_truth in ['yes', 'no'] and normalized_prediction != normalized_ground_truth:\n",
    "        return 0\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return 1.0 if (normalize_answer(prediction) == normalize_answer(ground_truth)) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde64dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Pred: Yes\n",
      "Ground Truth: yes\n",
      "Q: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\n",
      "Pred: Unknown\n",
      "Ground Truth: Chief of Protocol\n",
      "Q: What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\n",
      "Pred: Unknown\n",
      "Ground Truth: Animorphs\n",
      "Q: Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?\n",
      "Pred: No.\n",
      "Ground Truth: no\n",
      "Q: The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?\n",
      "Pred: Unknown\n",
      "Ground Truth: Greenwich Village, New York City\n",
      "Q: 2014 S/S is the debut album of a South Korean boy group that was formed by who?\n",
      "Pred: Unknown\n",
      "Ground Truth: YG Entertainment\n",
      "Q: Who was known by his stage name Aladin and helped organizations improve their performance as a consultant?\n",
      "Pred: Eenasul Fateh\n",
      "Ground Truth: Eenasul Fateh\n",
      "Q: The arena where the Lewiston Maineiacs played their home games can seat how many people?\n",
      "Pred: Unknown\n",
      "Ground Truth: 3,677 seated\n",
      "Q: Who is older, Annie Morton or Terry Richardson?\n",
      "Pred: Terry Richardson\n",
      "Ground Truth: Terry Richardson\n",
      "Q: Are Local H and For Against both from the United States?\n",
      "Pred: Yes.\n",
      "Ground Truth: yes\n",
      "Q: What is the name of the fight song of the university whose main campus is in Lawrence, Kansas and whose branch campuses are in the Kansas City metropolitan area?\n",
      "Pred: Unknown\n",
      "Ground Truth: Kansas Song\n",
      "Q: What screenwriter with credits for \"Evolution\" co-wrote a film starring Nicolas Cage and Téa Leoni?\n",
      "Pred: David Diamond\n",
      "Ground Truth: David Weissman\n",
      "Q: What year did Guns N Roses perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective?\n",
      "Pred: Unknown\n",
      "Ground Truth: 1999\n",
      "Q: Are Random House Tower and 888 7th Avenue both used for real estate?\n",
      "Pred: Yes.\n",
      "Ground Truth: no\n",
      "Q: The football manager who recruited David Beckham managed Manchester United during what timeframe?\n",
      "Pred: 1986 to 2013\n",
      "Ground Truth: from 1986 to 2013\n",
      "Q: Brown State Fishing Lake is in a country that has a population of how many inhabitants ?\n",
      "Pred: Unknown\n",
      "Ground Truth: 9,984\n",
      "Q: The Vermont Catamounts men's soccer team currently competes in a conference that was formerly known as what from 1988 to 1996?\n",
      "Pred: North Atlantic Conference\n",
      "Ground Truth: the North Atlantic Conference\n",
      "Q: Are Giuseppe Verdi and Ambroise Thomas both Opera composers ?\n",
      "Pred: Yes\n",
      "Ground Truth: yes\n",
      "Q: Roger O. Egeberg was Assistant Secretary for Health and Scientific Affairs during the administration of a president that served during what years?\n",
      "Pred: 1969-1974\n",
      "Ground Truth: 1969 until 1974\n",
      "Q: Which writer was from England, Henry Roth or Robert Erskine Childers?\n",
      "Pred: Robert Erskine Childers\n",
      "Ground Truth: Robert Erskine Childers DSC\n",
      "Q: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?\n",
      "Pred: Esteban Gutiérrez\n",
      "Ground Truth: Pedro Rodríguez\n",
      "Q: This singer of A Rather Blustery Day also voiced what hedgehog?\n",
      "Pred: Unknown\n",
      "Ground Truth: Sonic\n",
      "Q: Aside from the Apple Remote, what other device can control the program Apple Remote was originally designed to interact with?\n",
      "Pred: Unknown\n",
      "Ground Truth: keyboard function keys\n",
      "Q: Which performance act has a higher instrument to person ratio, Badly Drawn Boy or Wolf Alice? \n",
      "Pred: Badly Drawn Boy\n",
      "Ground Truth: Badly Drawn Boy\n",
      "Q: What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992?\n",
      "Pred: World's Best Goalkeeper\n",
      "Ground Truth: World's Best Goalkeeper\n",
      "Q: Who was the writer of These Boots Are Made for Walkin' and who died in 2007?\n",
      "Pred: Lee Hazlewood\n",
      "Ground Truth: Barton Lee Hazlewood\n",
      "Q: The 2011–12 VCU Rams men's basketball team, led by third year head coach Shaka Smart, represented Virginia Commonwealth University which was founded in what year?\n",
      "Pred: Unknown\n",
      "Ground Truth: 1838\n",
      "Q: Are both Dictyosperma, and Huernia described as a genus?\n",
      "Pred: Yes.\n",
      "Ground Truth: yes\n",
      "Q: Kaiser Ventures corporation was founded by an American industrialist who became known as the father of modern American shipbuilding?\n",
      "Pred: Yes\n",
      "Ground Truth: Henry J. Kaiser\n",
      "Q: What is the name for the adventure in \"Tunnels and Trolls\", a game designed by Ken St. Andre?\n",
      "Pred: Unknown\n",
      "Ground Truth: Arena of Khazan\n",
      "Q: When was Poison's album \"Shut Up, Make Love\" released?\n",
      "Pred: Unknown\n",
      "Ground Truth: 2000\n",
      "Q: Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?\n",
      "Pred: Japan\n",
      "Ground Truth: Fujioka, Gunma\n",
      "Q: Which  French ace pilot and adventurer fly L'Oiseau Blanc\n",
      "Pred: Charles Nungesser\n",
      "Ground Truth: Charles Eugène\n",
      "Q: Are Freakonomics and In the Realm of the Hackers both American documentaries?\n",
      "Pred: No.\n",
      "Ground Truth: no\n",
      "Q: Which band, Letters to Cleo or Screaming Trees, had more members?\n",
      "Pred: Screaming Trees\n",
      "Ground Truth: Letters to Cleo\n",
      "Q: Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of a civil war that ended when ?\n",
      "Pred: Unknown\n",
      "Ground Truth: October 1922\n",
      "Q: Seven Brief Lessons on Physics was written by an Italian physicist that has worked in France since what year?\n",
      "Pred: Unknown\n",
      "Ground Truth: 2000\n",
      "Q: The Livesey Hal War Memorial commemorates the fallen of which war, that had over 60 million casualties?\n",
      "Pred: World War II\n",
      "Ground Truth: World War II\n",
      "Q: Are both Elko Regional Airport and Gerald R. Ford International Airport located in Michigan?\n",
      "Pred: No.\n",
      "Ground Truth: no\n",
      "Q: Ralph Hefferline was a psychology professor at a university that is located in what city?\n",
      "Pred: New York\n",
      "Ground Truth: New York City\n",
      "Q: Which dog's ancestors include Gordon and Irish Setters: the Manchester Terrier or the Scotch Collie?\n",
      "Pred: Scotch Collie\n",
      "Ground Truth: Scotch Collie\n",
      "Q: Where is the company that Sachin Warrier worked for as a software engineer headquartered? \n",
      "Pred: Kochi\n",
      "Ground Truth: Mumbai\n",
      "Q: A Japanese manga series based on a 16 year old high school student Ichitaka Seto, is written and illustrated by someone born in what year?\n",
      "Pred: Unknown\n",
      "Ground Truth: 1962\n",
      "Q: The battle in which Giuseppe Arimondi lost his life secured what for Ethiopia?\n",
      "Pred: Ethiopian sovereignty.\n",
      "Ground Truth: sovereignty\n",
      "Q: Alfred Balk served as the secretary of the Committee on the Employment of Minority Groups in the News Media under which United States Vice President?\n",
      "Pred: Unknown\n",
      "Ground Truth: Nelson Rockefeller\n",
      "Q: A medieval fortress in Dirleton, East Lothian, Scotland borders on the south side of what coastal area?\n",
      "Pred: Yellowcraig\n",
      "Ground Truth: Yellowcraig\n",
      "Q: Who is the writer of this song that was inspired by words on a tombstone and was the first track on the box set Back to Mono?\n",
      "Pred: Phil Spector\n",
      "Ground Truth: Phil Spector\n",
      "Q: What type of forum did a former Soviet statesman initiate?\n",
      "Pred: Unknown\n",
      "Ground Truth: Organizations could come together to address global issues\n",
      "Q: Are Ferocactus and Silene both types of plant?\n",
      "Pred: Yes\n",
      "Ground Truth: yes\n",
      "Q: Which British first-generation jet-powered medium bomber was used in the South West Pacific theatre of World War II?\n",
      "Pred: Unknown\n",
      "Ground Truth: English Electric Canberra\n",
      "Q: Which year and which conference was the 14th season for this conference as part of the NCAA Division that the Colorado Buffaloes played in with a record of 2-6 in conference play?\n",
      "Pred: Unknown\n",
      "Ground Truth: 2009 Big 12 Conference\n",
      "Q: In 1991 Euromarché was bought by a chain that operated how any hypermarkets at the end of 2016?\n",
      "Pred: 1,462 hypermarkets\n",
      "Ground Truth: 1,462\n",
      "Q: What race track in the midwest hosts a 500 mile race eavery May?\n",
      "Pred: Unknown\n",
      "Ground Truth: Indianapolis Motor Speedway\n",
      "Q: In what city did the \"Prince of tenors\" star in a film based on an opera by Giacomo Puccini?\n",
      "Pred: Unknown\n",
      "Ground Truth: Rome\n",
      "Q: Ellie Goulding worked with what other writers on her third studio album, Delirium?\n",
      "Pred: Max Martin, Savan Kotecha, Ilya Salmanzadeh\n",
      "Ground Truth: Max Martin, Savan Kotecha and Ilya Salmanzadeh\n",
      "Q: Which Australian city founded in 1838 contains a boarding school opened by a Prime Minister of Australia and named after a school in London of the same name.\n",
      "Pred: Adelaide\n",
      "Ground Truth: Marion, South Australia\n",
      "Q: D1NZ is a series based on what oversteering technique?\n",
      "Pred: Drifting\n",
      "Ground Truth: Drifting\n",
      "Q: who is younger Keith Bostic or Jerry Glanville ?\n",
      "Pred: Keith Bostic\n",
      "Ground Truth: Keith Bostic\n",
      "Q: According to the 2001 census, what was the population of the city in which Kirton End is located?\n",
      "Pred: Unknown\n",
      "Ground Truth: 35,124\n",
      "Q: Are both Cypress and Ajuga genera?\n",
      "Pred: Yes.\n",
      "Ground Truth: no\n",
      "Q: What distinction is held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting?\n",
      "Pred: Muggsy Bogues was the shortest player in NBA history.\n",
      "Ground Truth: shortest player ever to play in the National Basketball Association\n",
      "Q: What is the name of the executive producer of the film that has a score composed by Jerry Goldsmith?\n",
      "Pred: Francis Ford Coppola\n",
      "Ground Truth: Ronald Shusett\n",
      "Q: Who was born earlier, Emma Bull or Virginia Woolf?\n",
      "Pred: Virginia Woolf\n",
      "Ground Truth: Adeline Virginia Woolf\n",
      "Q: What was the Roud Folk Song Index of the nursery rhyme inspiring What Are Little Girls Made Of?\n",
      "Pred: 821\n",
      "Ground Truth: 821\n",
      "Q: Scott Parkin has been a vocal critic of Exxonmobil and another corporation that has operations in how many countries ?\n",
      "Pred: More than 180 countries.\n",
      "Ground Truth: more than 70 countries\n",
      "Q: What WB supernatrual drama series was Jawbreaker star Rose Mcgowan best known for being in?\n",
      "Pred: Charmed\n",
      "Ground Truth: Charmed\n",
      "Q: Vince Phillips held a junior welterweight title by an organization recognized by what larger Hall of Fame?\n",
      "Pred: International Boxing Hall of Fame (IBHOF)\n",
      "Ground Truth: International Boxing Hall of Fame\n",
      "Q: What is the name of the singer who's song was released as the lead single from the album \"Confessions\", and that had popular song stuck behind for eight consecutive weeks?\n",
      "Pred: Unknown\n",
      "Ground Truth: Usher\n",
      "Q: who is the younger brother of The episode guest stars of The Hard Easy \n",
      "Pred: Unknown\n",
      "Ground Truth: Bill Murray\n",
      "Q: The 2017–18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sponsorship reasons?\n",
      "Pred: Carabao Cup\n",
      "Ground Truth: Carabao Cup\n",
      "Q: Which of Tara Strong major voice role in animated series is an American animated television series based on the DC Comics fictional superhero team, the \"Teen Titans\"?\n",
      "Pred: Teen Titans Go!\n",
      "Ground Truth: Teen Titans Go!\n",
      "Q: What is the inhabitant of the city where  122nd SS-Standarte was formed in2014\n",
      "Pred: Unknown\n",
      "Ground Truth: 276,170 inhabitants\n",
      "Q: What color clothing do people of the Netherlands wear during Oranjegekte or to celebrate the national holiday Koningsdag? \n",
      "Pred: Orange\n",
      "Ground Truth: orange\n",
      "Q: What was the name of the 1996 loose adaptation of William Shakespeare's \"Romeo & Juliet\" written by James Gunn?\n",
      "Pred: Tromeo and Juliet\n",
      "Ground Truth: Tromeo and Juliet\n",
      "Q: Robert Suettinger was the national intelligence officer under which former Governor of Arkansas?\n",
      "Pred: Bill Clinton\n",
      "Ground Truth: William Jefferson Clinton\n",
      "Q: What American professional Hawaiian surfer born 18 October 1992 won the Rip Curl Pro Portugal?\n",
      "Pred: John John Florence\n",
      "Ground Truth: John John Florence\n",
      "Q: What is the middle name of the actress who plays Bobbi Bacha in Suburban Madness?\n",
      "Pred: Unknown\n",
      "Ground Truth: Ann\n",
      "Q: Alvaro Mexia had a diplomatic mission with which tribe of indigenous people?\n",
      "Pred: Ais\n",
      "Ground Truth: Apalachees\n",
      "Q: What nationality were social anthropologists Alfred Gell and Edmund Leach?\n",
      "Pred: British\n",
      "Ground Truth: British\n",
      "Q: In which year was the King who made the 1925 Birthday Honours born?\n",
      "Pred: 1865\n",
      "Ground Truth: 1865\n",
      "Q: What is the county seat of the county where East Lempster, New Hampshire is located?\n",
      "Pred: Unknown\n",
      "Ground Truth: Newport\n",
      "Q: The Album Against the Wind was the 11th Album of a Rock singer Robert C Seger born may 6 1945. What was the Rock singers stage name ?\n",
      "Pred: Bob Seger\n",
      "Ground Truth: Bob Seger\n",
      "Q: Rostker v. Goldberg held that the practice of what way of filling armed forces vacancies was consitutional?\n",
      "Pred: Draft registration for men only.\n",
      "Ground Truth: Conscription\n",
      "Q: Handi-Snacks are a snack food product line sold by what American multinational confectionery, food, and beverage company that is based in Illinois?\n",
      "Pred: Mondelez International\n",
      "Ground Truth: Mondelez International, Inc.\n",
      "Q: What was the name of a woman from the book titled \"Their Lives: The Women Targeted by the Clinton Machine \" and was also a former white house intern?\n",
      "Pred: Monica Lewinsky\n",
      "Ground Truth: Monica Lewinsky\n",
      "Q: When was the American lawyer, lobbyist and political consultant who was a senior member of the presidential campaign of Donald Trump born?\n",
      "Pred: April 1, 1949\n",
      "Ground Truth: April 1, 1949\n",
      "Q: In what year was the novel that Lourenço Mutarelli based \"Nina\" on based first published?\n",
      "Pred: Unknown\n",
      "Ground Truth: 1866\n",
      "Q: Where are Teide National Park and Garajonay National Park located?\n",
      "Pred: Canary Islands, Spain\n",
      "Ground Truth: Canary Islands, Spain\n",
      "Q: How many copies of Roald Dahl's variation on a popular anecdote sold?\n",
      "Pred: Unknown\n",
      "Ground Truth: 250 million\n",
      "Q: What occupation do Chris Menges and Aram Avakian share?\n",
      "Pred: Film director\n",
      "Ground Truth: director\n",
      "Q: Andrew Jaspan was the co-founder of what not-for-profit media outlet?\n",
      "Pred: The Conversation\n",
      "Ground Truth: The Conversation\n",
      "Q: Which American film director hosted the 18th Independent Spirit Awards in 2002?\n",
      "Pred: John Waters\n",
      "Ground Truth: John Waters\n",
      "Q: Where does the hotel and casino located in which Bill Cosby's third album was recorded?\n",
      "Pred: Flamingo Hotel\n",
      "Ground Truth: Las Vegas Strip in Paradise\n",
      "Q: Do the drinks Gibson and Zurracapote both contain gin?\n",
      "Pred: No.\n",
      "Ground Truth: no\n",
      "Q: In what month is the annual documentary film festival, that is presented by the fortnightly published British journal of literary essays, held? \n",
      "Pred: March\n",
      "Ground Truth: March and April\n",
      "Q: Tysons Galleria is located in what county?\n",
      "Pred: Fairfax County\n",
      "Ground Truth: Fairfax County\n",
      "Q: Bordan Tkachuk was the CEO of a company that provides what sort of products?\n",
      "Pred: Unknown\n",
      "Ground Truth: IT products and services\n",
      "Q: Which filmmaker was known for animation, Lev Yilmaz or Pamela B. Green?\n",
      "Pred: Levni Yilmaz\n",
      "Ground Truth: Levni Yilmaz\n",
      "Q: In which city is the ambassador of the Rabat-Salé-Kénitra administrative region to China based?\n",
      "Pred: Unknown\n",
      "Ground Truth: Beijing\n",
      "Q: Are Yingkou and Fuding the same level of city?\n",
      "Pred: No.\n",
      "Ground Truth: no\n",
      "Metrics: {'n': 100, 'k': 5, 'EM': 0.38, 'F1': 0.4974939472292414}\n"
     ]
    }
   ],
   "source": [
    "def eval(ds, n, k=K_RETRIEVE):\n",
    "    idxs = list(range(min(n, len(ds)))) # first n examples\n",
    "\n",
    "    ems, f1s = [], []\n",
    "    detailed_results = []\n",
    "\n",
    "    for i in idxs:\n",
    "        ex = ds[i]\n",
    "        q = ex[\"question\"]\n",
    "        ground_truth = ex[\"answer\"]\n",
    "\n",
    "        # Predictions from your singlehop system\n",
    "        pred, passages = singlehop_answer(q, k=k)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"Pred: {pred}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "\n",
    "        em = exact_match_score(pred, ground_truth)\n",
    "        f1 = f1_score(pred, ground_truth)\n",
    "        ems.append(em)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "        # Collect structured data for Phoenix analysis\n",
    "        detailed_results.append({\n",
    "            \"index\": i,\n",
    "            \"question\": q,\n",
    "            \"prediction\": pred,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"em_score\": em,\n",
    "            \"f1_score\": f1,\n",
    "            \"retrieved_passages\": passages,\n",
    "        })\n",
    "\n",
    "    m = len(idxs) if idxs else 1\n",
    "    metrics = {\n",
    "        \"n\": len(idxs),\n",
    "        \"k\": k,\n",
    "        \"EM\": sum(ems)/m,\n",
    "        \"F1\": sum(f1s)/m,\n",
    "    }\n",
    "    return metrics, detailed_results\n",
    "\n",
    "# Run eval\n",
    "metrics, detailed_results = eval(ds_val, 100, k=K_RETRIEVE) # TODO: change N to ds_val size for full eval later\n",
    "print(\"Metrics:\", metrics)\n",
    "\n",
    "# Create DataFrame for Phoenix analysis\n",
    "eval_df = create_eval_dataframe(detailed_results, model_type=\"single_hop\")\n",
    "print(f\"\\nCreated evaluation DataFrame with {len(eval_df)} examples\")\n",
    "\n",
    "# Run Phoenix evaluations\n",
    "print(\"\\nRunning Phoenix evaluations (this may take a while)...\")\n",
    "evaluators = [hallucination_evaluator, completeness_evaluator]\n",
    "eval_df_with_scores = run_phoenix_evaluations(eval_df, evaluators)\n",
    "\n",
    "# Analyze errors\n",
    "print(\"\\nError Analysis:\")\n",
    "error_analysis = analyze_errors(eval_df_with_scores)\n",
    "for key, value in error_analysis.items():\n",
    "    if key != \"failed_examples_df\":\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Display sample of failed examples if any\n",
    "if \"failed_examples_df\" in error_analysis and len(error_analysis[\"failed_examples_df\"]) > 0:\n",
    "    print(f\"\\nSample of failed examples (showing first 5):\")\n",
    "    print(error_analysis[\"failed_examples_df\"].head())\n",
    "\n",
    "# Optionally save to CSV for later analysis\n",
    "# eval_df_with_scores.to_csv(\"single_hop_eval_results.csv\", index=False)\n",
    "# print(\"\\nResults saved to single_hop_eval_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a995f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoenix Error Analysis Setup\n",
    "import phoenix as px\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator,\n",
    "    ClassificationEvaluator,\n",
    "    LLM,\n",
    "    run_evaluator,\n",
    ")\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "def create_eval_dataframe(results_list, model_type=\"single_hop\"):\n",
    "    \"\"\"\n",
    "    Convert evaluation results list to pandas DataFrame for Phoenix analysis.\n",
    "    \n",
    "    Args:\n",
    "        results_list: List of dicts with evaluation results\n",
    "        model_type: \"single_hop\" or \"multi_hop\"\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with structured evaluation data\n",
    "    \"\"\"\n",
    "    df_data = []\n",
    "    for result in results_list:\n",
    "        row = {\n",
    "            \"model_type\": model_type,\n",
    "            \"index\": result[\"index\"],\n",
    "            \"question\": result[\"question\"],\n",
    "            \"prediction\": result[\"prediction\"],\n",
    "            \"ground_truth\": result[\"ground_truth\"],\n",
    "            \"em_score\": result[\"em_score\"],\n",
    "            \"f1_score\": result[\"f1_score\"],\n",
    "            \"context\": \"\\n\\n\".join(result[\"retrieved_passages\"]) if result[\"retrieved_passages\"] else \"\",\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ce74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phoenix Evaluators\n",
    "# Reuse existing LLM instance for evaluators\n",
    "eval_llm = OpenAIModel(model_name=CHAT_MODEL)\n",
    "\n",
    "# Hallucination Evaluator - checks if prediction is grounded in retrieved context\n",
    "hallucination_evaluator = HallucinationEvaluator(model=eval_llm)\n",
    "\n",
    "# Completeness Evaluator - assesses if prediction fully answers the question\n",
    "completeness_prompt = \"\"\"\n",
    "You are an expert at judging the completeness of a response to a query.\n",
    "Given a query and response, rate the completeness of the response.\n",
    "A response is complete if it fully answers all parts of the query.\n",
    "A response is partially complete if it only answers part of the query.\n",
    "A response is incomplete if it does not answer any part of the query or is not related to the query.\n",
    "\n",
    "Query: {query}\n",
    "Response: {response}\n",
    "\n",
    "Is the response complete, partially complete, or incomplete?\n",
    "\"\"\"\n",
    "\n",
    "completeness_evaluator = ClassificationEvaluator(\n",
    "    model=eval_llm,\n",
    "    name=\"completeness\",\n",
    "    prompt_template=completeness_prompt,\n",
    "    choices={\"complete\": 1.0, \"partially complete\": 0.5, \"incomplete\": 0.0},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phoenix Evaluations on DataFrame\n",
    "def run_phoenix_evaluations(df, evaluators):\n",
    "    \"\"\"\n",
    "    Run Phoenix evaluators on a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame with columns: question, prediction, context\n",
    "        evaluators: list of evaluator objects (not used directly, but kept for API consistency)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with evaluation scores added\n",
    "    \"\"\"\n",
    "    df_eval = df.copy()\n",
    "    \n",
    "    # Run hallucination evaluator\n",
    "    # HallucinationEvaluator expects: input (query), output (response), context\n",
    "    hallucination_results = []\n",
    "    print(\"Evaluating hallucination scores...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Create a single-row dataframe for this evaluation\n",
    "            eval_df = pd.DataFrame([{\n",
    "                \"input\": row[\"question\"],\n",
    "                \"output\": row[\"prediction\"],\n",
    "                \"context\": row[\"context\"] if row[\"context\"] else \"\",\n",
    "            }])\n",
    "            result = run_evaluator(hallucination_evaluator, dataframe=eval_df)\n",
    "            # Extract score from result\n",
    "            if len(result) > 0 and \"score\" in result.columns:\n",
    "                hallucination_results.append(result.iloc[0][\"score\"])\n",
    "            elif len(result) > 0:\n",
    "                # Try to get the first value if column name is different\n",
    "                hallucination_results.append(result.iloc[0].iloc[0])\n",
    "            else:\n",
    "                hallucination_results.append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating hallucination for row {idx}: {e}\")\n",
    "            hallucination_results.append(None)\n",
    "    \n",
    "    df_eval[\"hallucination_score\"] = hallucination_results\n",
    "    \n",
    "    # Run completeness evaluator\n",
    "    # ClassificationEvaluator expects: query, response\n",
    "    completeness_results = []\n",
    "    print(\"Evaluating completeness scores...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            eval_df = pd.DataFrame([{\n",
    "                \"query\": row[\"question\"],\n",
    "                \"response\": row[\"prediction\"],\n",
    "            }])\n",
    "            result = run_evaluator(completeness_evaluator, dataframe=eval_df)\n",
    "            # Extract score from result\n",
    "            if len(result) > 0 and \"score\" in result.columns:\n",
    "                completeness_results.append(result.iloc[0][\"score\"])\n",
    "            elif len(result) > 0:\n",
    "                # Try to get the first value if column name is different\n",
    "                completeness_results.append(result.iloc[0].iloc[0])\n",
    "            else:\n",
    "                completeness_results.append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating completeness for row {idx}: {e}\")\n",
    "            completeness_results.append(None)\n",
    "    \n",
    "    df_eval[\"completeness_score\"] = completeness_results\n",
    "    \n",
    "    return df_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ce318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis Functions\n",
    "def analyze_errors(df, f1_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze errors in evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with evaluation results including em_score, f1_score, hallucination_score, completeness_score\n",
    "        f1_threshold: F1 score threshold below which to consider an error\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with error analysis summary\n",
    "    \"\"\"\n",
    "    # Filter failed examples\n",
    "    failed = df[(df[\"em_score\"] == 0) | (df[\"f1_score\"] < f1_threshold)].copy()\n",
    "    \n",
    "    total = len(df)\n",
    "    num_failed = len(failed)\n",
    "    error_rate = num_failed / total if total > 0 else 0\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_examples\": total,\n",
    "        \"failed_examples\": num_failed,\n",
    "        \"error_rate\": error_rate,\n",
    "        \"avg_em_score\": df[\"em_score\"].mean(),\n",
    "        \"avg_f1_score\": df[\"f1_score\"].mean(),\n",
    "    }\n",
    "    \n",
    "    # Analyze error patterns if we have Phoenix scores\n",
    "    if \"hallucination_score\" in df.columns:\n",
    "        hallucination_errors = failed[failed[\"hallucination_score\"] == 0] if \"hallucination_score\" in failed.columns else pd.DataFrame()\n",
    "        analysis[\"hallucination_errors\"] = len(hallucination_errors)\n",
    "        analysis[\"hallucination_error_rate\"] = len(hallucination_errors) / num_failed if num_failed > 0 else 0\n",
    "    \n",
    "    if \"completeness_score\" in df.columns:\n",
    "        incomplete = failed[failed[\"completeness_score\"] < 1.0] if \"completeness_score\" in failed.columns else pd.DataFrame()\n",
    "        analysis[\"incomplete_answers\"] = len(incomplete)\n",
    "        analysis[\"incompleteness_rate\"] = len(incomplete) / num_failed if num_failed > 0 else 0\n",
    "    \n",
    "    # Error breakdown by type\n",
    "    if num_failed > 0:\n",
    "        analysis[\"failed_examples_df\"] = failed[[\"index\", \"question\", \"prediction\", \"ground_truth\", \"em_score\", \"f1_score\"]]\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def compare_models(single_hop_df, multi_hop_df):\n",
    "    \"\"\"\n",
    "    Compare error rates and performance between single-hop and multi-hop models.\n",
    "    \n",
    "    Args:\n",
    "        single_hop_df: DataFrame with single-hop evaluation results\n",
    "        multi_hop_df: DataFrame with multi-hop evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison metrics\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        \"single_hop\": {\n",
    "            \"avg_em\": single_hop_df[\"em_score\"].mean(),\n",
    "            \"avg_f1\": single_hop_df[\"f1_score\"].mean(),\n",
    "            \"error_rate\": (single_hop_df[\"em_score\"] == 0).mean(),\n",
    "        },\n",
    "        \"multi_hop\": {\n",
    "            \"avg_em\": multi_hop_df[\"em_score\"].mean(),\n",
    "            \"avg_f1\": multi_hop_df[\"f1_score\"].mean(),\n",
    "            \"error_rate\": (multi_hop_df[\"em_score\"] == 0).mean(),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    comparison[\"em_improvement\"] = comparison[\"multi_hop\"][\"avg_em\"] - comparison[\"single_hop\"][\"avg_em\"]\n",
    "    comparison[\"f1_improvement\"] = comparison[\"multi_hop\"][\"avg_f1\"] - comparison[\"single_hop\"][\"avg_f1\"]\n",
    "    comparison[\"error_reduction\"] = comparison[\"single_hop\"][\"error_rate\"] - comparison[\"multi_hop\"][\"error_rate\"]\n",
    "    \n",
    "    # Find questions where one model performs better\n",
    "    if len(single_hop_df) == len(multi_hop_df):\n",
    "        single_hop_better = (single_hop_df[\"em_score\"] > multi_hop_df[\"em_score\"]).sum()\n",
    "        multi_hop_better = (multi_hop_df[\"em_score\"] > single_hop_df[\"em_score\"]).sum()\n",
    "        comparison[\"single_hop_better_count\"] = single_hop_better\n",
    "        comparison[\"multi_hop_better_count\"] = multi_hop_better\n",
    "        comparison[\"tie_count\"] = len(single_hop_df) - single_hop_better - multi_hop_better\n",
    "    \n",
    "    return comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
